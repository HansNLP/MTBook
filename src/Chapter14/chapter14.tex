% !Mode:: "TeX:UTF-8"
% !TEX encoding = UTF-8 Unicode

%----------------------------------------------------------------------------------------
% 机器翻译：基础与模型
% Machine Translation: Foundations and Models
%
% Copyright 2020
% 肖桐(xiaotong@mail.neu.edu.cn) 朱靖波 (zhujingbo@mail.neu.edu.cn)
%----------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------
%    CONFIGURATIONS
%----------------------------------------------------------------------------------------

\renewcommand\figurename{图}%将figure改为图
\renewcommand\tablename{表}%将figure改为图
\chapterimage{fig-NEU-5.jpg} % Chapter heading image

%----------------------------------------------------------------------------------------
%	CHAPTER 14
%----------------------------------------------------------------------------------------

\chapter{神经机器翻译模型推断}

\parinterval 推断是神经机器翻译中的核心问题。训练时双语句子对模型是可见的，但是在推断阶段，模型需要根据输入的源语言句子预测译文，因此神经机器翻译的推断和训练过程有着很大的不同。特别是，推断系统往往对应着机器翻译实际部署的需要，因此机器翻译推断系统的精度和速度等因素也是实践中需要考虑的。

\parinterval 本章对神经机器翻译模型推断中的若干问题进行讨论。主要涉及三方面内容：

\begin{itemize}
\vspace{0.5em}
\item 神经机器翻译的基本问题，如推断方向、译文长度控制等。
\vspace{0.5em}
\item 神经机器翻译的推断加速方法，如轻量模型、非自回归模型等。
\vspace{0.5em}
\item 多模型集成推断。
\vspace{0.5em}
\end{itemize}
%----------------------------------------------------------------------------------------
%    NEW SECTION
%----------------------------------------------------------------------------------------
\section{面临的挑战}

\parinterval 神经机器翻译的推断是指：对于输入的源语言句子$\seq{x}$，使用已经训练好的模型找到最佳译文$\hat{\seq{y}}$的过程，其中$\hat{\seq{y}}=\arg\max\limits_{\seq{y}}\funp{P}(\seq{y}|\seq{x})$。这个过程也被称作解码。但是为了避免与神经机器翻译中编码器-解码器造成概念上的混淆，这里统一把翻译新句子的操作称作推断。以上这个过程是一个典型的搜索问题（见{\chaptertwo}），比如，可以使用贪婪搜索或者束搜索完成神经机器翻译的推断（见{\chapterten}）。

\parinterval 通用的神经机器翻译推断包括如下几步：

\begin{itemize}
\vspace{0.5em}
\item 对输入的源语言句子进行编码；
\vspace{0.5em}
\item 使用源语言句子的编码结果，在目标语言端自左向右逐词生成译文；
\vspace{0.5em}
\item 在目标语言的每个位置计算模型得分，同时进行剪枝；
\vspace{0.5em}
\item 当满足某种条件时终止搜索。
\vspace{0.5em}
\end{itemize}

\parinterval 这个过程与统计机器翻译中自左向右翻译是一样的（见{\chapterseven}），即在目标语言的每个位置，根据已经生成的部分译文和源语言的信息，生成下一个译文单词\upcite{Koehn2007Moses,DBLP:conf/amta/Koehn04}。它可以由两个模块实现\upcite{DBLP:conf/emnlp/StahlbergHSB17}：

\begin{itemize}
\vspace{0.5em}
\item {\small\sffamily\bfseries{预测模块}}，它根据已经生成的部分译文和源语言信息，预测下一个要生成的译文单词的概率分布\footnote{在统计机器翻译中，翻译的每一步也可以同时预测若干个连续的单词，即短语。在神经机器翻译中也有类似于生成短语的方
法，但是主流的方法还是按单词为单位进行生成。}。因此预测模块实际上就是一个模型打分装置；
\vspace{0.5em}
\item {\small\sffamily\bfseries{搜索模块}}，它会利用预测结果，对当前的翻译假设进行打分，并根据模型得分对翻译假设进行排序和剪枝。
\vspace{0.5em}
\end{itemize}

\parinterval 预测模块是由模型决定的，而搜索模块可以与模型无关。也就是说，不同的模型可以共享同一个搜索模块完成推断。比如，对于基于循环神经网络的模型，预测模块需要读入前一个状态的信息和前一个位置的译文单词，然后预测当前位置单词的概率分布；对于Transformer，预测模块需要对前面的所有位置做注意力运算，之后预测当前位置单词的概率分布。不过，这两个模型都可以使用同一个搜索模块。图\ref{fig:14-1}给出了这种架构的示意图。

%----------------------------------------------
\begin{figure}[htp]
\centering
\input{./Chapter14/Figures/figure-main-module}
\caption{神经机器翻译推断系统结构}
\label{fig:14-1}
\end{figure}
%----------------------------------------------

\parinterval 这是一个非常通用的框架，同样适用于统计机器翻译模型。因此，神经机器翻译推断中的很多问题与统计机器翻译是一致的，比如：束搜索的宽度、解码终止条件等等。

\parinterval 一般来说，设计机器翻译推断系统需要考虑三个因素：搜索的准确性、搜索的时延、搜索所需要的存储。通常，准确性是研究人员最关心的问题，比如可以通过增大搜索空间来找到模型得分更高的结果。而搜索的时延和存储消耗是实践中必须要考虑的问题，比如可以设计更小的模型和更高效的推断方法来提高系统的可用性。

\parinterval 虽然，上述问题在统计机器翻译中均有讨论，但是在神经机器翻译中又面临着新的挑战。

\begin{itemize}
\vspace{0.5em}
\item 搜索的基本问题在神经机器翻译中有着特殊的现象。比如，在统计机器翻译中，减少搜索错误是提升翻译品质的一种手段。但是神经机器翻译中，简单的减少搜索错误可能无法带来性能的提升，甚至会造成翻译品质的下降\upcite{li-etal-2018-simple,Stahlberg2019OnNS}；
\vspace{0.5em}
\item 搜索的时延很高，系统实际部署的成本很高。与统计机器翻译系统不同的是，神经机器翻译依赖大量的浮点运算。这导致神经机器翻译系统的推断会比统计机器翻译系统慢很多。虽然可以使用GPU来提高神经机器翻译的推断速度，但是也大大增加了成本；
\vspace{0.5em}
\item 神经机器翻译在优化过程中容易陷入局部最优，单模型的表现并不稳定。由于神经机器翻译优化的目标函数非常不光滑，每次训练得到的模型往往只是一个局部最优解。在新数据上使用这个局部最优模型进行推断时，模型的表现可能不稳定。
\vspace{0.5em}
\end{itemize}

\parinterval 研究人员也针对以上问题开展了大量的研究工作。在\ref{sec:14-2}节中，会对神经机器翻译推断中所涉及的一些基本问题进行讨论。虽然这些问题在统计机器翻译中也有涉及，但是在神经机器翻译中却有着不同的现象和解决思路。在\ref{sec:14-3}-\ref{sec:14-5}节中，会围绕如何改进神经机器翻译推断效率和怎样进行多模型融合这两个问题展开讨论。

%----------------------------------------------------------------------------------------
%    NEW SECTION
%----------------------------------------------------------------------------------------
\sectionnewpage
\section{基本问题}\label{sec:14-2}

\parinterval 下面将就神经机器翻译推断中的若干基本问题进行讨论，包括：推断方向、译文长度控制、搜索终止条件、译文多样性、搜索错误五个方面。

%----------------------------------------------------------------------------------------
%    NEW SUBSUB-SECTION
%----------------------------------------------------------------------------------------

\subsection{推断方向}\label{sec:14-2-1}

\parinterval 机器翻译有两种常用的推断方式\ \dash \ 自左向右推断和自右向左推断。自左向右推断符合现实世界中人类的语言使用规律，因为人在翻译一个句子时，总是习惯从句子开始的部分向后生成\footnote{有些语言中，文字是自右向左书写，这时自右向左推断更符合人类使用这种语言的习惯。}。不过，有时候人也会使用当前单词后面的译文信息。也就是说，翻译也需要“未来” 的文字信息。于是很容易想到使用自右向左的方式对译文进行生成。

\parinterval 以上两种推断方式在神经机器翻译中都有应用，对于源语言句子$\seq{x}=\{x_1,\dots,x_m\}$和目标语言句子$\seq{y}=\{y_1,\dots,y_n\}$，自左向右推断可以被描述为：
\begin{eqnarray}
\funp{P}(\seq{y}\vert\seq{x}) &=& \prod_{j=1}^n \funp{P}(y_j\vert\seq{y}_{<j},\seq{x})
\label{eq:14-1}
\end{eqnarray}
\parinterval 自右向左推断可以被描述为：
\begin{eqnarray}
\funp{P}(\seq{y}\vert\seq{x}) &=&\prod_{j=1}^n \funp{P}(y_{n+1-j}\vert\seq{y}_{>n+1-j},\seq{x})
\label{eq:14-2}
\end{eqnarray}

\noindent 其中，$\seq{y}_{<j}=\{y_1,\dots,y_{j-1}\}$，$\seq{y}_{>n+1-j}=\{y_{n+1-j},\dots,y_n\}$。可以看到，自左向右推断和自右向左推断本质上是一样的。{\chapterten}到{\chaptertwelve}均使用了自左向右的推断方法。自右向左推断比较简单的实现方式是：在训练过程中直接将双语数据中的目标语言句子进行反转，之后仍然使用原始的模型进行训练即可。在推断的时候，生成的目标语言词串也需要进行反转得到最终的译文。有时候，使用自右向左的推断方式会取得更好的效果\upcite{DBLP:conf/wmt/SennrichHB16}。不过更多情况下需要同时使用词串左端（历史）和右端（未来）的信息。有多种思路可以融合左右两端信息：

\begin{itemize}
\vspace{0.5em}
\item {\small\sffamily\bfseries{重排序}}\index{重排序}（Reranking）\index{Reranking}。可以用一个基础模型（比如自左向右的模型）得到每个源语言句子的$n$-best翻译结果，之后同时用基础模型的得分和自右向左模型的得分对$n$-best翻译结果进行重排序\upcite{Liu2016AgreementOT,DBLP:conf/wmt/SennrichHB16,DBLP:conf/wmt/LiLXLLLWZXWFCLL19}。也有研究人员利用最小贝叶斯风险的方法进行重排序\upcite{Stahlberg2018TheUO}。由于这类方法不会改变基础模型的翻译过程，因此相对“安全”，不会对系统性能造成副作用。
\vspace{0.5em}
\item {\small\sffamily\bfseries{双向推断}}\index{双向推断}（Bidirectional Inference）\index{Bidirectional Inference}。除了自左向右推断和自右向左推断，另一种方法让自左向右和自右向左模型同步进行，也就是同时考虑译文左侧和右侧的文字信息\upcite{DBLP:conf/aaai/ZhangSQLJW18,Zhou2019SynchronousBN,DBLP:conf/aaai/ZhangSQLJW18}。例如，可以同时对左侧和右侧生成的译文进行注意力计算，得到当前位置的单词预测结果。这种方法能够更加充分地融合双向翻译的优势。
\vspace{0.5em}
\item {\small\sffamily\bfseries{多阶段推断}}\index{多阶段推断}（Multi-stage Inference）\index{Multi-stage Inference}。在第一阶段，通过一个基础模型生成一个初步的翻译结果。在第二阶段，同时使用第一阶段生成的翻译结果和源语言句子，进一步生成更好的译文\upcite{Li2017EnhancedNM,ElMaghraby2018EnhancingTF,Geng2018AdaptiveMD}。由于第一阶段的结果已经包含了完整的译文信息，因此在第二阶段中，系统实际上已经同时使用了整个译文串的两端信息。上述过程可以扩展为迭代式的译文生成方法，配合掩码等技术，可以在生成每个译文单词时，同时考虑左右两端的上下文信息\upcite{Lee2018DeterministicNN,Gu2019LevenshteinT,Guo2020JointlyMS}。
\vspace{0.5em}
\end{itemize}

\parinterval 不论是自左向右推断还是自右向左推断，本质上都是在对上下文信息进行建模。此外，研究人员也提出了许多新的译文生成策略，比如，从中部向外生成\upcite{DBLP:conf/nips/MehriS18}、按源语言顺序生成\upcite{Stahlberg2018AnOS}、基于插入的方式生成\upcite{Stern2019InsertionTF,stling2017NeuralMT}等。或者将翻译问题松弛化为一个连续空间模型的优化问题，进而在推断的过程中同时使用译文左右两端的信息\upcite{Geng2018AdaptiveMD}。

\parinterval 最近，以BERT 为代表的预训练语言模型已经证明，一个单词的“历史” 和“未来” 信息对于生成当前单词都是有帮助的\upcite{devlin2019bert}。类似的观点也在神经机器翻译编码器设计中得到验证。比如，在基于循环神经网络的模型中，经常同时使用自左向右和自右向左的方式对源语言句子进行编码；在Transformer 模型中，编码器会使用整个句子的信息对每一个源语言位置进行表示。因此，神经机器翻译的推断采用类似的策略是有其合理性的。

%----------------------------------------------------------------------------------------
%    NEW SUBSUB-SECTION
%----------------------------------------------------------------------------------------

\subsection{译文长度控制}

\parinterval 机器翻译推断的一个特点是译文长度需要额外的机制进行控制\upcite{Kikuchi2016ControllingOL,Takase2019PositionalET,Murray2018CorrectingLB,Sountsov2016LengthBI}。这是因为机器翻译在建模时仅考虑了将训练样本（即标准答案）上的损失最小化，但是推断的时候会看到从未见过的样本，而且这些未见样本占据了样本空间的绝大多数。该问题会导致的一个现象是：直接使用训练好的模型会翻译出长度短得离谱的译文。神经机器翻译模型使用单词概率的乘积表示整个句子的翻译概率，它天然就倾向生成短译文，因为概率为大于0小于1的常数，短译文会使用更少的概率因式相乘，倾向于得到更高的句子得分，而模型只关心每个目标语言位置是否被正确预测，对于译文长度没有考虑。统计机器翻译模型中也存在译文长度不合理的问题，解决该问题的常见策略是在推断过程中引入译文长度控制机制\upcite{Koehn2007Moses}。神经机器翻译也借用了类似的思想来控制译文长度，有以下几种方法：

\begin{itemize}
\vspace{0.5em}
\item {\small\sffamily\bfseries{长度惩罚因子}}。用译文长度来归一化翻译概率是最常用的方法：对于源语言句子$\seq{x}$和译文句子$\seq{y}$，模型得分$\textrm{score}(\seq{x},\seq{y})$的值会随着译文$\seq{y}$ 的长度增大而减小。为了避免此现象，可以引入一个长度惩罚函数$\textrm{lp}(\seq{y})$，并定义模型得分如公式\eqref{eq:14-12}所示：
\begin{eqnarray}
\textrm{score}(\seq{x},\seq{y}) &=& \frac{\log \funp{P}(\seq{y}\vert\seq{x})}{\textrm{lp}(\seq{y})}
\label{eq:14-12}
\end{eqnarray}

通常$\textrm{lp}(\seq{y})$随译文长度$\vert\seq{y}\vert$的增大而增大，因此这种方式相当于对$\log \funp{P}(\seq{y}\vert\seq{x})$按长度进行归一化\upcite{Jean2015MontrealNM}。$\textrm{lp}(\seq{y})$的定义方式有很多，表\ref{tab:14-1}列出了一些常用的形式，其中$\alpha$是需要人为设置的参数。

%----------------------------------------------------------------------------------------------------
\begin{table}[htp]
\centering
\caption{长度惩罚因子$\textrm{lp}(\seq{y})$的定义（$|\seq{y}|$表示译文长度）}
\label{tab:14-1}
\small
\begin{tabular}{l | l l l}
\rule{0pt}{15pt}     名称 & $\textrm{lp}(\seq{y})$  \\
\hline
\rule{0pt}{15pt}     句子长度 &  $\textrm{lp}(\seq{y}) = {\vert\seq{y}\vert}^{\alpha}$ \\
\rule{0pt}{15pt}     GNMT惩罚因子 &  $\textrm{lp}(\seq{y}) = \frac{{(5+\vert\seq{y}\vert)}^{\alpha}}{{(5+1)}^{\alpha}}$ \\
\rule{0pt}{15pt}     指数化长度惩罚因子 &  $\textrm{lp}(\seq{y}) = \alpha \cdot \log(\vert\seq{y}\vert)$ \\
\end{tabular}
\end{table}
%----------------------------------------------------------------------------------------------------
\vspace{0.5em}
\item {\small\sffamily\bfseries{译文长度范围约束}}。为了让译文的长度落在合理的范围内，神经机器翻译的推断也会设置一个译文长度约束\upcite{Vaswani2018Tensor2TensorFN,KleinOpenNMT}。令$[a,b]$表示一个长度范围，可以定义:
\begin{eqnarray}
a &=& \omega_{\textrm{low}}\cdot |\seq{x}| \label{eq:14-3}\\
b &=& \omega_{\textrm{high}}\cdot |\seq{x}| \label{eq:14-4}
\end{eqnarray}
\vspace{0.5em}
\noindent 其中，$\omega_{\textrm{low}}$和$\omega_{\textrm{high}}$分别表示译文长度的下限和上限，比如，很多系统中设置为$\omega_{\textrm{low}}=1/2$，$\omega_{\textrm{high}}=2$，表示译文至少有源语言句子一半长，最多有源语言句子两倍长。$\omega_{\textrm{low}}$和$\omega_{\textrm{high}}$的设置对推断效率影响很大，$\omega_{\textrm{high}}$可以被看作是一个推断的终止条件，最理想的情况是$\omega_{\textrm{high}} \cdot |\seq{x}|$恰巧就等于最佳译文的长度，这时没有浪费任何计算资源。反过来的一种情况，$\omega_{\textrm{high}} \cdot |\seq{x}|$远大于最佳译文的长度，这时很多计算都是无用的。为了找到长度预测的准确率和召回率之间的平衡，一般需要大量的实验最终确定$\omega_{\textrm{low}}$和$\omega_{\textrm{high}}$。当然，利用统计模型预测$\omega_{\textrm{low}}$ 和$\omega_{\textrm{high}}$也是非常值得探索的方向，比如基于繁衍率的模型\upcite{Gu2017NonAutoregressiveNM,Feng2016ImprovingAM}。
\vspace{0.5em}
\item {\small\sffamily\bfseries{覆盖度模型}}。译文长度过长或过短的问题，本质上对应着 {\small\sffamily\bfseries{过翻译}}\index{过翻译}（Over Translation）\index{Over Translation}和{\small\sffamily\bfseries{欠翻译}}\index{欠翻译}（Under Translation）\index{Under Translation}的问题\upcite{Yang2018OtemUtemOA}。这两种问题出现的原因主要在于：神经机器翻译没有对过翻译和欠翻译建模，即机器翻译覆盖度问题\upcite{TuModeling}。针对此问题，最常用的方法是在推断的过程中引入一个度量覆盖度的模型。比如，使用GNMT 覆盖度模型定义模型得分\upcite{Wu2016GooglesNM}，如下：
\begin{eqnarray}
\textrm{score}(\seq{x},\seq{y}) &=& \frac{\log \funp{P}(\seq{y} | \seq{x})}{\textrm{lp}(\seq{y})} + \textrm{cp}(\seq{x},\seq{y}) \label {eq:14-5}\\
\textrm{cp}(\seq{x},\seq{y}) &=& \beta \cdot \sum_{i=1}^{|\seq{x}|} \log(\textrm{min} (\sum_{j}^{|\seq{y}|} a_{ij} , 1))
\label{eq:14-6}
\end{eqnarray}

\noindent 其中，$\textrm{cp}(\seq{x},\seq{y}) $表示覆盖度模型，它度量了译文对源语言每个单词的覆盖程度。$\textrm{cp}(\seq{x},\seq{y}) $的定义中，$\beta$是一需要自行设置的超参数，$a_{ij}$表示源语言第$i$个位置与译文 第$j$个位置的注意力权重，这样$\sum \limits_{j}^{|\seq{y}|} a_{ij}$就可以用来衡量源语言第$i$个单词中的信息被翻译的程度，如果它大于1，表明翻译多了；如果小于1，表明翻译少了。公式\eqref{eq:14-6}会惩罚那些欠翻译的翻译假设。对覆盖度模型的一种改进形式是\upcite{li-etal-2018-simple}：
\begin{eqnarray}
\textrm{cp}(\seq{x},\seq{y}) &=& \sum_{i=1}^{|\seq{x}|} \log( \textrm{max} ( \sum_{j}^{|\seq{y}|} a_{ij},\beta))
\label{eq:14-7}
\end{eqnarray}

\noindent 公式\eqref{eq:14-7}将公式\eqref{eq:14-6}中的向下截断方式改为了向上截断。这样，模型可以对过翻译（或重复翻译）有更好的建模能力。不过，这个模型需要在开发集上细致地调整$\beta$，也带来了一定的额外工作量。此外，也可以将这种覆盖度单独建模并进行参数化，与翻译模型一同训练\upcite{Mi2016CoverageEM,TuModeling,Kazimi2017CoverageFC}。这样可以得到更加精细的覆盖度模型。
\vspace{0.5em}
\end{itemize}

%----------------------------------------------------------------------------------------
%    NEW SUBSUB-SECTION
%----------------------------------------------------------------------------------------

\subsection{搜索终止条件}

\parinterval 在机器翻译推断中，何时终止搜索是一个非常基础的问题。如{\chaptertwo}所述，系统研发者一方面希望尽可能遍历更大的搜索空间，找到更好的结果，另一方面也希望在尽可能短的时间内得到结果。这时搜索的终止条件就是一个非常关键的指标。在束搜索中有很多终止条件可以使用，比如，在生成一定数量的译文之后就终止搜索，或者当最佳译文与排名第二的译文之间的分数差距超过一个阈值时就终止搜索等。

\parinterval 在统计机器翻译中，搜索的终止条件相对容易设计。因为所有的翻译结果都可以用相同步骤的搜索过程生成，比如，在CYK推断中搜索的步骤仅与构建的分析表大小有关。在神经机器翻译中，这个问题要更加复杂。当系统找到一个完整的译文之后，可能还有很多译文没有被生成完，这时就面临着一个问题\ \dash \ 如何决定是否继续搜索。

\parinterval 针对这些问题，研究人员设计了很多新的方法。比如，可以在束搜索中使用启发性信息让搜索尽可能早地停止，同时保证搜索结果是“最优的”\upcite{DBLP:conf/emnlp/HuangZM17}。也可以将束搜索建模为优化问题\upcite{Wiseman2016SequencetoSequenceLA,DBLP:conf/emnlp/Yang0M18}，进而设计出新的终止条件\upcite{Ma2019LearningTS}。很多开源机器翻译系统也都使用了简单有效的终止条件，比如，在OpenNMT 系统中当搜索束中当前最好的假设生成了完整的译文搜索就会停止\upcite{KleinOpenNMT}，在RNNSearch系统中当找到预设数量的译文时搜索就会停止，同时在这个过程中会不断减小搜索束的大小\upcite{bahdanau2014neural}。

\parinterval 实际上，设计搜索终止条件反映了搜索时延和搜索精度的一种折中\upcite{Eisner2011LearningST,Jiang2012LearnedPF}。在很多应用中，这个问题会非常关键。比如，在同声传译中，对于输入的长文本，何时开始翻译、何时结束翻译都是十分重要的\upcite{Zheng2020OpportunisticDW,Ma2019STACLST}。在很多线上翻译应用中，翻译结果的响应不能超过一定的时间，这时就需要一种{\small\sffamily\bfseries{时间受限的搜索}}\index{时间受限的搜索}（Time-constrained Search）\index{Time-constrained Search}策略\upcite{DBLP:conf/emnlp/StahlbergHSB17}。

%----------------------------------------------------------------------------------------
%    NEW SUBSUB-SECTION
%----------------------------------------------------------------------------------------

\subsection{译文多样性}

\parinterval 机器翻译系统的输出并不仅限于单个译文。很多情况下，需要多个译文。比如，译文重排序中通常就需要系统的$n$-best输出，在交互式机器翻译中也往往需要提供多个译文供用户选择\upcite{Peris2017InteractiveNM,Peris2018ActiveLF}。但是，无论是统计机器翻译还是神经机器翻译，都面临一个同样的问题：$n$-best输出中的译文十分相似。实例\ref{eg:14-1}就展示了一个神经机器翻译输出的多个翻译结果，可以看到这些译文的区别很小。这个问题也被看做是机器翻译缺乏译文多样性的问题\upcite{Gimpel2013ASE,Li2016MutualIA,DBLP:conf/emnlp/DuanLXZ09,DBLP:conf/acl/XiaoZZW10,xiao2013bagging}。

\begin{example}
源语言句子：我们/期待/安理会/尽早/就此/作出/决定/ 。

\qquad\ 机器译文\ \,1\ ：We look forward to the Security Council making a decision on this

\hspace{8.3em}as soon as possible .

\qquad\ 机器译文\ \,2\ ：We look forward to the Security Council making a decision on this

\hspace{8.3em}issue as soon as possible .

\qquad\ 机器译文\ 3\ ：We hope that the Security Council will make a decision on this

\hspace{8.4em}issue as soon as possible .

\label{eg:14-1}
\end{example}

\parinterval  机器翻译输出缺乏多样性会带来很多问题。一个直接的问题是在重排序时很难选择到更好的译文，因为所有候选都没有太大的差别。此外，当需要利用$n$-best输出来表示翻译假设空间时，缺乏多样性的译文也会使得翻译后验概率的估计不够准确，造成建模的偏差。在一些模型训练方法中，这种后验概率估计的偏差也会造成较大的影响\upcite{DBLP:conf/acl/ShenCHHWSL16}。从人工翻译的角度，同一个源语言句子的译文应该是多样的，因此过于相似的译文也无法反映足够多的翻译现象。

\parinterval 因此增加译文多样性成为了机器翻译中一个有价值的研究方向。在统计机器翻译中就有很多尝试\upcite{DBLP:conf/emnlp/DuanLXZ09,DBLP:conf/acl/XiaoZZW10,xiao2013bagging}。主要思路是通过加入一些“扰动”让翻译模型的行为发生变化，进而得到区别更大的译文。类似的方法也同样适用于神经机器翻译。例如，可以在推断过程中引入额外的模型，用于惩罚出现相似译文的情况\upcite{Li2016ADO,Li2016MutualIA}。也可以在翻译模型中引入新的隐含变量或者加入新的干扰，进而控制多样性译文的输出\upcite{He2018SequenceTS,Shen2019MixtureMF,Wu2020GeneratingDT}。类似地，也可以利用模型中局部结构的多样性来生成多样的译文\upcite{Sun2020GeneratingDT}。除了考虑每个译文之间的多样性，也可以对译文进行分组，之后增加不同组之间的多样性\upcite{Vijayakumar2016DiverseBS}。

%----------------------------------------------------------------------------------------
%    NEW SUBSUB-SECTION
%----------------------------------------------------------------------------------------

\subsection{搜索错误}

\parinterval 机器翻译的错误分为两类：搜索错误和模型错误。搜索错误是指由于搜索算法的限制，即使潜在的搜索空间中有更好的解，模型也无法找到。比较典型的例子是，在对搜索结果进行剪枝的时候，如果剪枝过多，找到的结果很有可能不是最优的，这时就出现了搜索错误。而模型错误则是指由于模型学习能力的限制，即使搜索空间中存在最优解，模型也无法将该解排序在前面。

\parinterval 在统计机器翻译中，搜索错误可以通过减少剪枝进行缓解。比较简单的方式是增加搜索束宽度，这往往会带来一定的性能提升\upcite{Xiao2016ALA}。也可以对搜索问题进行单独建模，以保证学习到的模型出现更少的搜索错误\upcite{Liu2014SearchAwareTF,Yu2013MaxViolationPA}。但是，在神经机器翻译中，这个问题却表现出不同的现象：在很多神经机器翻译系统中，随着搜索束的增大，系统的BLEU不升反降。图\ref{fig:14-3}展示了神经机器翻译系统中BLEU随搜索束大小的变化曲线，这里为了使该图更加规整直观，横坐标处将束大小进行了取对数操作。这个现象与传统的常识是相违背的，因此也有一些研究尝试解释这个现象\upcite{Stahlberg2019OnNS,Niehues2017AnalyzingNM}。

%----------------------------------------------------------------------
\begin{figure}[htp]
\centering
 \input{./Chapter14/Figures/figure-beamsize-bleu}
\caption{一个神经机器翻译系统中搜索束大小对BLEU的影响\upcite{DBLP:conf/aclnmt/KoehnK17}}
\label{fig:14-3}
\end{figure}
%----------------------------------------------------------------------
\parinterval 在实验中，研究人员发现增加搜索束的大小会导致翻译生成的结果变得更短。他们将这个现象归因于：神经机器翻译的建模基于局部归一的最大似然估计，增加搜索束的大小，会导致更多的模型错误\upcite{Sountsov2016LengthBI,Murray2018CorrectingLB,StahlbergNeural}。此外，也有研究人员把这种翻译过短的现象归因于搜索错误\upcite{Stahlberg2019OnNS}： 由于搜索时所面临的搜索空间是十分巨大的，因此搜索时可能无法找到模型定义的“最好”的译文，在某种意义上，这也体现了训练和推断不一致的问题（见{\chapterthirteen}）。一种解决该问题的思路是从“训练和推断行为不一致”的角度切入。比如，为了解决曝光偏置问题\upcite{Ranzato2016SequenceLT}，可以让系统使用前面步骤的预测结果作为预测下一个词所需要的历史信息，而不是依赖于标准答案\upcite{Bengio2015ScheduledSF,Zhang2019BridgingTG}。为了解决训练和推断目标不一致的问题，可以在训练的时候模拟推断的行为，同时让模型训练的目标与评价系统的标准尽可能一致\upcite{DBLP:conf/acl/ShenCHHWSL16}。

\parinterval 此外，还有其它方法解决增大搜索束造成的翻译品质下降的问题。比如，可以通过对结果重排序来缓解这个问题\upcite{DBLP:conf/emnlp/Yang0M18}，也可以通过设计更好的覆盖度模型来生成长度更加合理的译文\upcite{li-etal-2018-simple}。从这个角度说，上述问题的成因也较为复杂，因此需要同时考虑模型错误和搜索错误。

%----------------------------------------------------------------------------------------
%    NEW SECTION
%----------------------------------------------------------------------------------------
\sectionnewpage
\section{轻量模型}\label{sec:14-3}

\parinterval 翻译速度和翻译精度之间的平衡是机器翻译系统研发中的常见问题。即使是以提升翻译品质为目标的任务（如用BLEU进行评价），也不得不考虑翻译速度的影响。比如，在很多任务中会构造伪数据，该过程涉及对大规模单语数据的翻译；无监督机器翻译中也会频繁地使用神经机器翻译系统构造训练数据。在这些情况下，如果翻译速度过慢会增大实验的周期。从应用的角度看，在很多场景下翻译速度甚至比翻译品质更重要。比如，在线翻译和一些小设备上的机器翻译系统都需要保证相对低的翻译时延，以满足用户体验的最基本要求。虽然，我们希望能有一套又好又快的翻译系统，但是现实的情况是：往往需要通过牺牲一些翻译品质来换取翻译速度的提升。下面就列举一些常用的神经机器翻译轻量模型和加速方法。这些方法通常应用在神经机器翻译的解码器上，因为相比编码器，解码器是推断过程中最耗时的部分。

%----------------------------------------------------------------------------------------
%    NEW SUBSUB-SECTION
%----------------------------------------------------------------------------------------

\subsection{输出层的词汇选择}

\parinterval 神经机器翻译需要对输入和输出的单词进行分布式表示。但是，由于真实的词表通常很大，因此计算并保存这些单词的向量表示会消耗较多的计算和存储资源。特别是对于基于Softmax 的输出层，大词表的计算十分耗时。虽然可以通过BPE 和限制词汇表规模的方法降低输出层计算的负担\upcite{DBLP:conf/acl/SennrichHB16a}，但是为了获得可接受的翻译品质，词汇表也不能过小，因此输出层的计算代价仍然很高。

\parinterval 通过改变输出层的结构，可以一定程度上缓解这个问题\upcite{DBLP:conf/acl/JeanCMB15}。一种比较简单的方法是对可能输出的单词进行筛选，即词汇选择。这里，可以利用类似于统计机器翻译的翻译表，获得每个源语言单词最可能的译文。在翻译过程中，利用注意力机制找到每个目标语言位置对应的源语言位置，之后获得这些源语言单词最可能的翻译候选。之后，Softmax 只需要在这个有限的翻译候选单词集合上进行计算，大大降低了输出层的计算量。尤其对于CPU 上的系统，这个方法往往会带来明显的速度提升。图\ref{fig:14-4}对比了标准Softmax与词汇选择方法中的Softmax。

%----------------------------------------------
\begin{figure}[htp]
\centering
\input{./Chapter14/Figures/figure-different-softmax}
\caption{标准Softmax vs 基于词汇选择的Softmax}
\label{fig:14-4}
\end{figure}
%----------------------------------------------
\parinterval 实际上，词汇选择也是一种典型的处理大词表的方法（见\chapterthirteen）。这种方法最大的优点在于，它可以与其它方法结合，比如与BPE等方法结合。本质上，这种方法与传统基于统计的机器翻译中的短语表剪枝有类似之处\upcite{DBLP:conf/emnlp/ZensSX12,DBLP:conf/emnlp/JohnsonMFK07,DBLP:conf/emnlp/LingGTB12}，当翻译候选过多的时候，可以根据翻译候选对候选集进行剪枝。这种技术已经在统计机器翻译系统中得到成功应用。


%----------------------------------------------------------------------------------------
%    NEW SUBSUB-SECTION
%----------------------------------------------------------------------------------------

\subsection{消除冗余计算}

\parinterval 消除不必要的计算是加速机器翻译系统的另一种方法。比如，在统计机器翻译时代，假设重组就是一种典型的避免冗余计算的手段（见{\chapterseven}）。在神经机器翻译中，消除冗余计算的一种简单有效的方法是对解码器的注意力结果进行缓存。以Transformer为例，在生成每个译文时，Transformer 模型会对当前位置之前的所有位置进行自注意力操作，但是这些计算里只有和当前位置相关的计算是“新” 的，前面位置之间的注意力结果已经在之前的解码步骤里计算过，因此可以对其进行缓存。

\parinterval 此外，由于Transformer 模型较为复杂，还存在很多冗余。比如，Transformer 的每一层会包含自注意力机制、层正则化、残差连接、前馈神经网络等多种不同的结构。同时，不同结构之间还会包含一些线性变换。多层Transformer模型会更加复杂。但是，这些层可能在做相似的事情，甚至有些计算根本就是重复的。图\ref{fig:14-5}中展示了解码器自注意力和编码-解码注意力中不同层的注意力权重的相似性，这里的相似性利用Jensen-Shannon散度进行度量\upcite{61115}。可以看到，自注意力中，2-6层之间的注意力权重的分布非常相似。编码-解码注意力也有类似的现象，临近的层之间有非常相似的注意力权重。这个现象说明：在多层神经网络中有些计算是冗余的，因此很自然的想法是消除这些冗余使得机器翻译变得更“轻”。

%----------------------------------------------
\begin{figure}[htp]
\centering
\input{./Chapter14/Figures/figure-weight-similarity}
\caption{自注意力和编码-解码注意力中不同层之间注意力权重的相似性（深色表示相似）}
\label{fig:14-5}
\end{figure}
%----------------------------------------------
\parinterval 一种消除冗余计算的方法是将不同层的注意力权重进行共享，这样顶层的注意力权重可以复用底层的注意力权重\upcite{Xiao2019SharingAW}。在编码-解码注意力中，由于注意力机制中输入的Value 都是一样的\footnote{在Transformer解码器，编码-解码注意力输入的Value是编码器的输出，因此是相同的（见\chaptertwelve）。}，甚至可以直接复用前一层注意力计算的结果。图\ref{fig:14-6}给出了不同方法的对比，其中$S$表示注意力权重，$A$表示注意力模型的输出。可以看到，使用共享的思想，可以大大减少冗余的计算。

%----------------------------------------------
\begin{figure}[htp]
\centering
\input{./Chapter14/Figures/figure-comparison-of-different-attention-method}
\caption{标准的多层自注意力、共享自注意力、共享编码-解码注意力方法的对比\upcite{Xiao2019SharingAW}}
\label{fig:14-6}
\end{figure}
%----------------------------------------------

\parinterval 另一种方法是对不同层的参数进行共享。这种方法虽然不能带来直接的提速，但是可以大大减小模型的体积。比如，可以重复使用同一层的参数完成多层的计算。极端情况下，六层网络可以只使用一层网络的参数\upcite{DBLP:conf/aaai/DabreF19}。不过，在深层模型中（层数> 20），浅层部分的差异往往较大，而深层（远离输入）之间的相似度会更高。这时可以考虑对深层的部分进行更多的共享。

\parinterval 减少冗余计算也代表了一种剪枝的思想。本质上，这类方法利用了模型参数的稀疏性假设\upcite{Narang2017BlockSparseRN,Gale2019TheSO}：一部分参数对模型整体的行为影响不大，因此可以直接被抛弃掉。这类方法也被使用在神经机器翻译模型的不同部分。比如，对于Transformer模型，也有研究发现多头注意力中的有些头是有冗余的\upcite{Michel2019AreSH}，因此可以直接对其进行剪枝\upcite{DBLP:journals/corr/abs-1905-09418}。

%----------------------------------------------------------------------------------------
%    NEW SUBSUB-SECTION
%----------------------------------------------------------------------------------------

\subsection{轻量解码器及小模型}

\parinterval 在推断时，神经机器翻译的解码器是最耗时的，因为每个目标语言位置需要单独输出单词的分布，同时在搜索过程中每一个翻译假设都要被扩展成多个翻译假设，进一步增加了计算量。因此，提高推断速度的一种思路是使用更加轻量的解码器加快翻译假设的生成速度\upcite{Hinton2015Distilling,Munim2019SequencelevelKD}。

\parinterval 比较简单的做法是把解码器的网络变得更“浅”、更“窄”。所谓浅网络是指使用更少的层构建神经网络，比如，使用3 层，甚至1 层网络的Transformer 解码器。所谓窄网络是指将网络中某些层中神经元的数量减少。不过，直接训练这样的小模型会造成翻译品质下降。这时会考虑使用知识蒸馏等技术来提升小模型的品质（见{\chapterthirteen}）。

\parinterval 化简Transformer 解码器的神经网络也可以提高推断速度。比如，可以使用平均注意力机制代替原始Transformer 中的自注意力机制\upcite{DBLP:journals/corr/abs-1805-00631}，也可以使用运算更轻的卷积操作代替注意力模块\upcite{Wu2019PayLA}。前面提到的基于共享注意力机制的模型也是一种典型的轻量模型\upcite{Xiao2019SharingAW}。这些方法本质上也是对注意力模型结构的优化，这类思想在近几年也受到了很多关注 \upcite{Kitaev2020ReformerTE,Katharopoulos2020TransformersAR,DBLP:journals/corr/abs-2006-04768}，在{\chapterfifteen}也会有进一步讨论。

\parinterval 此外，使用异构神经网络也是一种平衡精度和速度的有效方法。在很多研究中发现，基于Transformer 的编码器对翻译品质的影响更大，而解码器的作用会小一些。因此，一种想法是使用速度更快的解码器结构，比如，用基于循环神经网络的解码器代替Transformer模型中基于注意力机制的解码器\upcite{Chen2018TheBO}。这样，既能发挥Transformer 在编码上的优势，同时也能利用循环神经网络在解码器速度上的优势。使用类似的思想，也可以用卷积神经网络等结构进行解码器的设计。

\parinterval 针对轻量级Transformer模型的设计也包括层级的结构剪枝，这类方法试图通过跳过某些操作或者某些层来降低计算量。典型的相关工作是样本自适应神经网络结构，如 FastBERT\upcite{Liu2020FastBERTAS}、Depth Adaptive Transformer\upcite{Elbayad2020DepthAdaptiveT}等，与传统的Transformer的解码过程不同，这类神经网络结构在推断时不需要计算全部的解码层，而是根据输入自动选择模型的部分层进行计算，达到加速和减少参数量的目的。

%----------------------------------------------------------------------------------------
%    NEW SUBSUB-SECTION
%----------------------------------------------------------------------------------------

\subsection{批量推断}

\parinterval 深度学习时代下，使用GPU已经成为大规模使用神经网络方法的前提。特别是对于机器翻译这样的复杂任务，GPU 的并行运算能力会带来明显的速度提升。为了充分利用GPU 的并行能力，可以同时对多个句子进行翻译，即{\small\sffamily\bfseries{批量推断}}\index{批量推断}（Batch Inference）\index{Batch Inference}。

\parinterval 在\chapterten 已经介绍了神经机器翻译中批量处理的基本概念，其实现并不困难，不过有两方面问题需要注意：

\begin{itemize}
\vspace{0.5em}
\item {\small\sffamily\bfseries{批次生成策略}}。对于源语言文本预先给定的情况，通常是按句子长度组织每个批次，即：把长度相似的句子放到一个批次里。这样做的好处是可以尽可能保证一个批次中的内容是“满” 的，否则如果句长差异过大会造成批次中有很多位置用占位符填充，产生无用计算。对于实时翻译的情况，批次的组织较为复杂。在机器翻译系统的实际应用中，由于有翻译时延的限制，可能待翻译句子未积累到标准批次数量就要进行翻译。常见的做法是，设置一个等待的时间，在同一个时间段中的句子可以放到一个批次中（或者几个批次中）。对于高并发的情况，也可以考虑使用不同的{\small\sffamily\bfseries{桶}}\index{桶}（Bucket\index{Bucket}）保存不同长度范围的句子，之后将同一个桶中的句子进行批量推断。这个问题在{\chaptereighteen}中还会做进一步讨论。
\vspace{0.5em}
\item {\small\sffamily\bfseries{批次大小的选择}}。一个批次中的句子数量越多，GPU 设备的利用率越高，系统吞吐越大。但是，一个批次中所有句子翻译结束后才能拿到翻译结果，因此批次中有些句子即使已经翻译结束也要等待其它没有完成的句子。也就是说，从单个句子来看，批次越大翻译的延时越长，这也导致在翻译实时性要求较高的场景中，不能使用过大的批次。而且，大批次对GPU 显存的消耗更大，因此也需要根据具体任务合理选择批次大小。为了说明这些问题，图\ref{fig:14-7}展示了不同批次大小下的时延和显存消耗。
\vspace{0.5em}
\end{itemize}

%----------------------------------------------
\begin{figure}[htp]
\centering
\input{./Chapter14/Figures/figure-batch-time-mem}
\caption{神经机器翻译系统在不同批次大小下的延时和显存消耗}
\label{fig:14-7}
\end{figure}
%----------------------------------------------

%----------------------------------------------------------------------------------------
%    NEW SUBSUB-SECTION
%----------------------------------------------------------------------------------------

\subsection{低精度运算}

\parinterval 降低运算强度也是计算密集型任务的加速手段之一。标准的神经机器翻译系统大多基于单精度浮点运算。从计算机的硬件发展看，单精度浮点运算还是很“重” 的。当计算能容忍一些精度损失的时候，可以考虑降低运算精度来达到加速的目的。比如：

\begin{itemize}
\vspace{0.5em}
\item {\small\sffamily\bfseries{半精度浮点运算}}。半精度浮点运算是随着近几年GPU 技术发展而逐渐流行的一种运算方式。简单来说，半精度的表示要比单精度需要更少的存储单元，所表示的浮点数范围也相应的变小。不过，实践中已经证明神经机器翻译中的许多运算用半精度计算就可以满足对精度的要求。因此，直接使用半精度运算可以大大加速系统的训练和推断进程，同时对翻译品质的影响很小。不过，需要注意的是，在分布式训练时，由于参数服务器需要对多个计算节点上的梯度进行累加，因此保存参数时仍然会使用单精度浮点以保证多次累加之后不会造成过大的精度损失。
\vspace{0.5em}
\item {\small\sffamily\bfseries{整型运算}}。整型运算是一种比浮点运算“轻” 很多的运算。无论是芯片占用面积、能耗还是处理单次运算的时钟周期数，相比浮点运算，整型运算都有着明显的优势。不过，整数的表示和浮点数有着很大的不同。一个基本问题是，整数是不连续的，因此无法准确地刻画浮点数中很小的小数。对于这个问题，一种解决方法是利用“量化+ 反量化+ 缩放” 的策略让整型运算达到与浮点运算近似的效果\upcite{DBLP:journals/corr/abs-1906-00532,DBLP:conf/cvpr/JacobKCZTHAK18,DBLP:journals/corr/abs-1910-10485}。所谓“量化” 就是把一个浮点数离散化为一个整数，“反量化” 是这个过程的逆过程。由于浮点数可能超出整数的范围，因此会引入一个缩放因子：在量化前将浮点数缩放到整数可以表示的范围，反量化前再缩放回原始浮点数的表示范围。这种方法在理论上可以带来很好的加速效果。不过由于量化和反量化的操作本身也有时间消耗，而且在不同处理器上的表现差异较大。因此不同实现方式带来的加速效果并不相同，需要通过实验测算。
\vspace{0.5em}
\item {\small\sffamily\bfseries{低精度整型运算}}。使用更低精度的整型运算是进一步加速的手段之一。比如使用16 位整数、8 位整数，甚至4 位整数在理论上都会带来速度的提升，如表\ref{tab:14-3}所示。不过，并不是所有处理器都支持低精度整型的运算。开发这样的系统，一般需要硬件和特殊低精度整型计算库的支持。而且相关计算大多是在CPU 上实现，应用会受到一定的限制。
\vspace{0.5em}
\end{itemize}

%----------------------------------------------
\begin{table}[htp]
\centering
\caption{不同计算精度的运算速度对比\protect\footnotemark}
\begin{tabular}{ l | l  l l l l}
\rule{0pt}{13pt} 指标 & FP32 &INT32 &INT16 &INT8 &INT4 \\ \hline
\rule{0pt}{13pt} 速度 & 1$\times$ & 3$\sim$4$\times$ & $\approx$4$\times$ & 4$\sim$6$\times$ & $\approx$8$\times$
\end{tabular}
\label{tab:14-3}
\end{table}
\footnotetext{表中比较了几种通用数据类型的乘法运算速度，不同硬件和架构上不同类型的数据的计算速度略有不同。总体来看整型数据类型和浮点型数据相比具有显著的计算速度优势，INT4相比于FP32数据类型的计算最高能达到8倍的速度提升。}
%--------------------------------------

\parinterval 实际上，低精度运算的另一个好处是可以减少模型存储的体积。比如，如果要把机器翻译模型作为软件的一部分打包存储，这时可以考虑用低精度的方式保存模型参数，使用时再恢复成原始精度的参数。值得注意的是，参数的离散化表示（比如整型表示）的一个极端例子是{\small\sffamily\bfseries{二值网络}}\index{二值网络}（Binarized Neural Networks）\index{Binarized Neural Networks}\upcite{DBLP:conf/nips/HubaraCSEB16}，即只用−1和+1 表示神经网络的每个参数\footnote{也存在使用0或1表示神经网络参数的二值网络。}。二值化可以被看作是一种极端的量化手段。不过，这类方法还没有在机器翻译中得到大规模验证。

%----------------------------------------------------------------------------------------
%    NEW SECTION
%----------------------------------------------------------------------------------------
\sectionnewpage
\section{非自回归翻译}

\parinterval 目前大多数神经机器翻译模型都使用自左向右逐词生成译文的策略，即第$j$个目标语言单词的生成依赖于先前生成的$j-1$ 个词。这种翻译方式也被称作{\small\sffamily\bfseries{自回归解码}}\index{自回归解码}（Autoregressive Decoding）\index{Autoregressive Decoding}。虽然以Transformer为代表的模型使得训练过程高度并行化，加快了训练速度。但由于推断过程自回归的特性，模型无法同时生成译文中的所有单词，导致模型的推断过程非常缓慢，这对于神经机器翻译的实际应用是个很大的挑战。因此，如何设计一个在训练和推断阶段都能够并行化的模型是目前研究的热点之一。

%----------------------------------------------------------------------------------------
%    NEW SUBSUB-SECTION
%----------------------------------------------------------------------------------------

\subsection{自回归 vs 非自回归}

\parinterval 目前主流的神经机器翻译的推断是一种{\small\sffamily\bfseries{自回归翻译}}\index{自回归翻译}（Autoregressive Translation）\index{Autoregressive Translation}过程。所谓自回归是一种描述时间序列生成的方式：对于目标序列$\seq{y}=\{y_1,\dots,y_n\}$，如果$j$时刻状态$y_j$的生成依赖于之前的状态$\{y_1,\dots,y_{j-1}\}$，而且$y_j$与$\{y_1,\dots,y_{j-1}\}$构成线性关系，那么称目标序列$\seq{y}$的生成过程是自回归的。神经机器翻译借用了这个概念，但是并不要求$y_j$与$\{y_1,\dots,y_{j-1}\}$构成线性关系，\ref{sec:14-2-1}节提到的自左向右翻译模型和自右向左翻译模型都属于自回归翻译模型。自回归模型在机器翻译任务上也有很好的表现，特别是配合束搜索往往能够有效地寻找近似最优译文。但是，由于解码器的每个步骤必须顺序地而不是并行地运行，自回归翻译模型会阻碍不同译文单词生成的并行化。特别是在GPU 上，翻译的自回归性会大大降低计算的并行度和设备利用率。

\parinterval 对于这个问题，研究人员也考虑移除翻译的自回归性，进行{\small\sffamily\bfseries{非自回归翻译}}\index{非自回归翻译}（Non-Autoregressive Translation，NAT）\index{Non-Autoregressive Translation}\upcite{Gu2017NonAutoregressiveNM}。一个简单的非自回归翻译模型将问题建模为公式\eqref{eq:14-9}：
\begin{eqnarray}
\funp{P}(\seq{y}|\seq{x}) &=& \prod_{j=1}^n {\funp{P}(y_j|\seq{x})}
\label{eq:14-9}
\end{eqnarray}

\parinterval 对比公式\eqref{eq:14-1}可以看出，公式\eqref{eq:14-9}中位置$j$上的输出$y_j$只依赖于输入句子$\seq{x}$，与其它位置上的输出无关。于是，可以并行生成所有位置上的${y_j}$。理想情况下，这种方式一般可以带来几倍甚至十几倍的速度提升。

%----------------------------------------------------------------------------------------
%    NEW SUB-SECTION
%----------------------------------------------------------------------------------------

\subsection{非自回归翻译模型的结构}

\parinterval 在介绍非自回归模型的具体结构之前，先来看看如何实现一个简单的非自回归翻译模型。这里用标准的Transformer来举例。首先为了一次性生成所有的词，需要丢弃解码器对未来信息屏蔽的矩阵，从而去掉模型的自回归性。此外，还要考虑生成译文的长度。在自回归模型中，每步的输入是上一步解码出的结果，当预测到终止符<eos>时，序列的生成就自动停止了，然而非自回归模型却没有这样的特性，因此还需要一个长度预测器来预测出其长度，之后再用这个长度得到每个位置的表示，将其作为解码器的输入，进而完成整个序列的生成。

%----------------------------------------------------------------------
\begin{figure}[htp]
\centering
 \input{./Chapter14/Figures/figure-non-autoregressive}
\caption{自回归翻译模型 vs 非自回归翻译模型}
\label{fig:14-12}
\end{figure}
%----------------------------------------------------------------------

\parinterval 图\ref{fig:14-12}对比了自回归翻译模型和简单的非自回归翻译模型。可以看到这种自回归翻译模型可以一次性生成完整的译文。不过，高并行性也带来了翻译品质的下降。比如，在IWSLT 英德等数据上的BLEU[\%] 值只有个位数，而现在最好的自回归模型已经能够达到30左右的BLEU得分。这是因为每个位置词的预测只依赖于源语言句子$\seq{x}$，使得预测不准确。需要注意的是，图\ref{fig:14-12}(b)中将位置编码作为非自回归模型解码器的输入只是一个最简单的例子，在真实的系统中，非自回归解码器的输入一般是拷贝的源语言句子词嵌入与位置编码的融合。



\parinterval 完全独立地对每个词建模，会出现什么问题呢？来看一个例子，将汉语句子“干/得/好/！”翻译成英文，可以翻译成“Good job !”或者“Well done !”。假设生成这两种翻译的概率是相等的，即一半的概率是“Good job !”，另一半的概率是“Well done !”。由于非自回归模型的条件独立性假设，推断时第一个词“Good”和“Well”的概率是差不多大的，如果第二个词“job”和“done”的概率也差不多大，会使得模型生成出“Good done !”或者“Well job !”这样错误的翻译，如图\ref{fig:14-13}所示。这便是影响句子质量的关键问题，称之为{\small\sffamily\bfseries{多峰问题}}\index{多峰问题}（Multimodality Problem）\index{Multimodality Problem}\upcite{Gu2017NonAutoregressiveNM}。如何有效处理非自回归模型中的多峰问题  是提升非自回归模型质量的关键。

%----------------------------------------------------------------------
\begin{figure}[htp]
\centering
 \input{./Chapter14/Figures/figure-multi-modality}
\caption{非自回归模型中的多峰问题}
\label{fig:14-13}
\end{figure}
%----------------------------------------------------------------------

\parinterval 因此，非自回归翻译的研究大多集中在针对以上问题的求解。有三个角度：使用繁衍率预测译文长度、使用句子级知识蒸馏来降低学习难度、使用自回归模型进行翻译候选打分。下面将依次对这些方法进行介绍。

%----------------------------------------------------------------------------------------
%    NEW SUBSUB-SECTION
%----------------------------------------------------------------------------------------

\subsubsection{1. 基于繁衍率的非自回归模型}

\parinterval 图\ref{fig:14-14}给出了基于繁衍率的Transformer非自回归模型的结构\upcite{Gu2017NonAutoregressiveNM}，由三个模块组成:编码器，解码器，繁衍率预测器。类似于标准的Transformer模型，这里编码器和解码器都完全由前馈神经网络和多头注意力模块组成。唯一的不同是解码器中新增了位置注意力模块（图\ref{fig:14-14}中被红色虚线框住的模块），用于更好的捕捉目标语言端的位置信息。

\parinterval 繁衍率预测器的一个作用是预测整个译文句子的长度，以便并行地生成所有译文单词。可以通过对每个源语言单词计算繁衍率来估计最终译文的长度。具体来说，繁衍率指的是：根据每个源语言单词预测出其对应的目标语言单词的个数（见\chaptersix），如图\ref{fig:14-14}所示，翻译过程中英语单词“We”对应一个汉语单词“我们”，其繁衍率为1。于是，可以得到源语言句子对应的繁衍率序列（图\ref{fig:14-14}中的数字1\ 1\ 2\ 0\ 1），最终译文长度则由源语言单词的繁衍率之和决定。之后将源语言单词按该繁衍率序列进行拷贝，在图中的例子中，将“We”、“totally”、“.”拷贝一次，将"accept"、“it”分别拷贝两次和零次，就得到了最终解码器的输入“We totally accept accept .”。在模型训练阶段，繁衍率序列可以通过外部词对齐工具得到， 用于之后训练繁衍率预测器。但由于外部词对齐系统会出现错误，因此在模型收敛之后，可以对繁衍率预测器进行额外的微调。

%----------------------------------------------------------------------
\begin{figure}[htp]
\centering
 \input{./Chapter14/Figures/figure-reproduction-rate}
\caption{基于繁衍率的非自回归模型}
\label{fig:14-14}
\end{figure}
%----------------------------------------------------------------------

\parinterval 实际上，使用繁衍率的另一个好处在于可以缓解多峰问题。因为，繁衍率本身可以看作是模型的一个隐变量。使用这个隐变量本质上是在对可能的译文空间进行剪枝，因为只有一部分译文满足给定的繁衍率序列。从这个角度说，在翻译率的作用下，不同单词译文组合的情况变少了，因此多峰问题也就被缓解了。

\parinterval 另外，在每个解码器层中还新增了额外的位置注意力模块，该模块与其它部分中使用的多头注意力机制相同。其仍然基于$\mathbi{Q}$、$\mathbi{K}$、$\mathbi{V}$之间的计算（见{\chaptertwelve}），只是把位置编码作为$\mathbi{Q}$ 和$\mathbi{K}$, 解码器端前一层的输出作为$\mathbi{V}$。这种方法提供了更强的位置信息。

%----------------------------------------------------------------------------------------
%    NEW SUBSUB-SECTION
%----------------------------------------------------------------------------------------

\subsubsection{2. 句子级知识蒸馏}

\parinterval 知识蒸馏的基本思路是把教师模型的知识传递给学生模型，让学生模型可以更好地学习（见\chapterthirteen）。通过这种方法，可以降低非自回归模型的学习难度。具体来说，可以让自回归模型作为“教师”，非自回归模型作为“学生”。把自回归神经机器翻译模型生成的句子作为新的训练样本，送给非自回归机器翻译模型进行学习\upcite{Lee2018DeterministicNN,Zhou2020UnderstandingKD,Guo2020FineTuningBC}。有研究发现自回归模型生成的结果的“确定性”更高，也就是不同句子中相同源语言片段翻译的多样性相对低一些\upcite{Gu2017NonAutoregressiveNM}。虽然从人工翻译的角度看，这可能并不是理想的译文，但是使用这样的译文可以在一定程度上缓解多峰问题。因为，经过训练的自回归模型会始终将相同的源语言句子翻译成相同的译文。这样得到的数据集噪声更少，能够降低非自回归模型学习的难度。此外，相比人工标注的译文，自回归模型输出的译文更容易让模型进行学习，这也是句子级知识蒸馏有效的原因之一。

%----------------------------------------------------------------------------------------
%    NEW SUBSUB-SECTION
%----------------------------------------------------------------------------------------

\subsubsection{3.自回归模型打分}

\parinterval 通过采样不同的繁衍率序列，可以得到多个不同的翻译候选。之后，把这些不同的译文再交给自回归模型来评分，选择一个最好的结果作为最终的翻译结果。通常，这种方法能够很有效地提升非自回归翻译模型的译文质量，并且保证较高的推断速度\upcite{Gu2017NonAutoregressiveNM,Wei2019ImitationLF,Guo2019NonAutoregressiveNM,Wang2019NonAutoregressiveMT,Ma2019FlowSeqNC}。但是，缺点是需要同时部署自回归和非自回归两套系统。

%----------------------------------------------------------------------------------------
%    NEW SUBSECTION
%----------------------------------------------------------------------------------------

\subsection{更好的训练目标}

\parinterval 虽然非自回归翻译可以显著提升翻译速度，但是很多情况下其翻译质量还是低于传统的自回归翻译\upcite{Gu2017NonAutoregressiveNM,Kaiser2018FastDI,Guo2020FineTuningBC}。因此，很多工作致力于缩小自回归模型和非自回归模型的性能差距\upcite{Ran2020LearningTR,Tu2020ENGINEEI,Shu2020LatentVariableNN}。

\parinterval 一种直接的方法是层级知识蒸馏\upcite{Li2019HintBasedTF}。由于自回归模型和非自回归模型的结构相差不大，因此可以将翻译质量更高的自回归模型作为“教师”，通过给非自回归模型提供监督信号，使其逐块地学习前者的分布。研究人员发现了两点非常有意思的现象：1）非自回归模型容易出现“重复翻译”的现象，这些相邻的重复单词所对应的位置的隐藏状态非常相似。2）非自回归模型的注意力分布比自回归模型的分布更加尖锐。这两点发现启发了研究人员使用自回归模型中的隐层状态和注意力矩阵等中间表示来指导非自回归模型的学习过程。可以计算两个模型隐层状态的距离以及注意力矩阵的KL散度\footnote{KL散度即相对熵。}，将它们作为额外的损失指导非自回归模型的训练。类似的做法也出现在基于模仿学习的方法中\upcite{Wei2019ImitationLF}，它也可以被看作是对自回归模型不同层行为的模拟。不过，基于模仿学习的方法会使用更复杂的模块来完成自回归模型对非自回归模型的指导，比如，在自回归模型和非自回归模型中都使用一个额外的神经网络，用于传递自回归模型提供给非自回归模型的层级监督信号。

\parinterval 此外，也可以使用基于正则化因子的方法\upcite{Wang2019NonAutoregressiveMT}。非自回归模型的翻译结果中存在着两种非常严重的错误：重复翻译和不完整的翻译。重复翻译问题是因为解码器隐层状态中相邻的两个位置过于相似，因此翻译出来的单词也一样。对于不完整翻译，即欠翻译问题，通常是由于非自回归模型在翻译的过程中丢失了一些源语言句子的信息。针对这两个问题，可以通过在相邻隐层状态间添加相似度约束来计算一个重构损失。具体实践时，对于翻译$\seq{x}\to\seq{y}$，通过一个反向的自回归模型再将$\seq{y}$翻译成$\seq{x'}$，最后计算$\seq{x}$与$\seq{x'}$的差异性作为损失。


%----------------------------------------------------------------------------------------
%    NEW SUBSECTION
%----------------------------------------------------------------------------------------

\subsection{引入自回归模块}

\parinterval 非自回归翻译消除了序列生成过程中不同位置预测结果间的依赖，在每个位置都进行独立的预测，但这反而会导致翻译质量显著下降，因为缺乏不同单词间依赖关系的建模。因此，也有研究聚焦于在非自回归模型中添加一些自回归组件。

\parinterval 一种做法是将句法信息作为目标语言句子的框架\upcite{Akoury2019SyntacticallyST}。具体来说，先自回归地预测出一个目标语言的句法块序列，将句法块作为序列信息的抽象，然后根据句法块序列非自回归地生成所有目标语言单词。如图\ref{fig:14-21}所示,该模型由一个编码器和两个解码器组成。其中编码器和第一个解码器与标准的Transformer模型相同，用来自回归地预测句法树信息；第二个解码器将第一个解码器的句法信息作为输入，之后再非自回归地生成整个译文。在训练过程中，通过使用外部句法分析器获得对句法预测任务的监督信号。虽然可以简单地让模型预测整个句法树，但是这种方法会显著增加自回归步骤的数量，从而增大时间开销。因此，为了维持句法信息与解码时间的平衡，这里预测一些由句法标记和子树大小组成的块标识符（如VP3）而不是整个句法树。关于基于句法的神经机器翻译模型在{\chapterfifteen}还会有进一步讨论。

%----------------------------------------------
\begin{figure}[htp]
\centering
\input{./Chapter14/Figures/figure-syntax}
\caption{基于句法结构的非自回归模型}
\label{fig:14-21}
\end{figure}
%----------------------------------------------

\parinterval 另一种做法是半自回归地生成译文\upcite{Wang2018SemiAutoregressiveNM}。如图\ref{fig:14-20}所示，自回归模型从左到右依次生成译文，具有“最强”的自回归性；而非自回归模型完全独立的生成每个译文单词，具有“最弱”的自回归性；半自回归模型则是将整个译文分成$k$个块，在块内执行非自回归解码，在块间则执行自回归的解码，能够在每个时间步并行产生多个连续的单词。通过调整块的大小，半自回归模型可以灵活的调整为自回归翻译（当$k$等于1）和非自回归翻译（当$k$大于等于最大的译文长度）。

%----------------------------------------------
\begin{figure}[htp]
\centering
\input{./Chapter14/Figures/figure-3vs}
\caption{自回归、半自回归和非自回归解码对比\upcite{Wang2018SemiAutoregressiveNM}}
\label{fig:14-20}
\end{figure}
%----------------------------------------------

\parinterval 还有一种做法引入了轻量级的自回归调序模块\upcite{Ran2019GuidingNN}。为了解决非自回归模型解码搜索空间过大的问题，可以使用调序技术在相对较少的翻译候选上进行自回归模型的计算。如图\ref{fig:14-22}所示，该方法对源语言句子进行重新排列转换成由源语言单词组成但位于目标语言结构中的伪译文，然后将伪译文进一步转换成目标语言以获得最终的翻译。其中，这个调序模块可以是一个轻量自回归模型，例如，一层的循环神经网络。

%----------------------------------------------------------------------
\begin{figure}[htp]
\centering
 \input{./Chapter14/Figures/figure-reranking}
\caption{引入调序模块的非自回归模型}
\label{fig:14-22}
\end{figure}
%----------------------------------------------------------------------

%----------------------------------------------------------------------------------------
%    NEW SUBSECTION
%----------------------------------------------------------------------------------------

\subsection{基于迭代精化的非自回归翻译模型}

\parinterval 如果一次并行生成整个序列，往往很难捕捉单词之间的关系，而且即便生成了错误的译文单词，这类方法也无法修改。针对这些问题，也可以使用迭代式的生成方式\upcite{Lee2018DeterministicNN,Ghazvininejad2019MaskPredictPD,Kasai2020NonAutoregressiveMT}。这种方法放弃了一次生成最终的译文句子，而是将解码出的译文再重新送给解码器，在每次迭代中来改进之前生成的译文单词，可以理解为句子级的自回归模型。这样做的好处在于，每次迭代的过程中可以利用已经生成的部分翻译结果，来指导其它部分的生成。

\parinterval 图\ref{fig:14-18}展示了这种方法的简单示例。它拥有一个编码器和$N$个解码器。编码器首先预测出译文的长度，然后将输入$\seq{x}$按照长度复制出$\seq{x'}$作为第一个解码器的输入，之后生成$\seq{y}^{[1]}$作为第一轮迭代的输出。接下来再把$\seq{y}^{[1]}$输入给第二个解码器，然后输出$\seq{y}^{[2]}$，以此类推。那么迭代到什么时候结束呢？一种简单的做法是提前制定好迭代次数，这种方法能够自主地对生成句子的质量和效率进行平衡。另一种称之为“自适应”的方法，具体是通过计算当前生成的句子与上一次生成句子之间的变化量来判断是否停止，例如，使用杰卡德相似系数作为变化量函数\footnote{杰卡德相似系数是衡量有限样本集之间的相似性与差异性的一种指标，杰卡德相似系数值越大，样本相似度越高。}。另外，需要说明的是，图\ref{fig:14-18}中是使用多个解码器的一种逻辑示意。真实的系统仅需要一个解码器，并运行多次，就达到了迭代精化的目的。

%----------------------------------------------
\begin{figure}[htp]
\centering
\input{./Chapter14/Figures/figure-iteration}
\caption{基于迭代精化的非自回归翻译模型运行示例}
\label{fig:14-18}
\end{figure}
%----------------------------------------------

\parinterval 除了使用上一个步骤的输出，当前解码器的输入还可以使用了添加噪声的正确目标语言句子\upcite{Lee2018DeterministicNN}。另外，对于译文长度的预测，也可以使用编码器的输出单独训练一个独立的长度预测模块，这种方法也推广到了目前大多数非自回归模型上。

\parinterval 另一种方法借鉴了BERT的思想\upcite{devlin2019bert}，称为Mask-Predict\upcite{Ghazvininejad2019MaskPredictPD}。类似于BERT中的<CLS>标记，该方法在源语言句子的最前面加上了一个特殊符号<LEN>作为输入，用来预测目标句的长度$n$。之后，将特殊符<Mask>（与BERT中的<Mask>有相似的含义）复制$n$次作为解码器的输入，然后用非自回归的方式生成所有的译文单词。这样生成的翻译可能是比较差的，因此可以将第一次生成的这些词中不确定（即生成概率比较低）的一些词“擦”掉，依据剩余的译文单词以及源语言句子重新进行预测，不断迭代，直到满足停止条件为止。图\ref{fig:14-19}给出了一个示例。

%----------------------------------------------
\begin{figure}[htp]
\centering
\input{./Chapter14/Figures/figure-mask-predict}
\caption{Mask-Predict方法的运行示例}
\label{fig:14-19}
\end{figure}
%----------------------------------------------

%----------------------------------------------------------------------------------------
%    NEW SECTION
%----------------------------------------------------------------------------------------
\sectionnewpage
\section{多模型集成}\label{sec:14-5}

\parinterval 在机器学习领域，把多个模型融合成一个模型是提升系统性能的一种有效方法。比如，在经典的AdaBoost 方法中\upcite{DBLP:journals/jcss/FreundS97}，用多个“弱” 分类器构建的“强” 分类器可以使模型在训练集上的分类错误率无限接近0。类似的思想也被应用到机器翻译中\upcite{DBLP:conf/acl/XiaoZZW10,DBLP:conf/icassp/SimBGSW07,DBLP:conf/acl/RostiMS07,DBLP:conf/wmt/RostiZMS08}，被称为{\small\sffamily\bfseries{系统融合}}\index{系统融合}（System Combination）\index{System Combination}。在各种机器翻译比赛中，系统融合已经成为经常使用的技术之一。由于许多模型融合方法都是在推断阶段完成，因此此类方法开发的代价较低。

\parinterval 广义上来讲，使用多个特征组合的方式都可以被看作是一种模型的融合。融合多个神经机器翻译系统的方法有很多，可以分为假设选择、局部预测融合、译文重组三类，下面分别进行介绍。

%----------------------------------------------------------------------------------------
%    NEW SUBSUB-SECTION
%----------------------------------------------------------------------------------------

\subsection{假设选择}

\parinterval {\small\sffamily\bfseries{假设选择}}\index{假设选择}（Hypothesis Selection）\index{Hypothesis Selection}是最简单的系统融合方法\upcite{DBLP:conf/emnlp/DuanLXZ09}。其思想是：给定一个翻译假设集合，综合多个模型对每一个翻译假设进行打分，之后选择得分最高的假设作为结果输出。

\parinterval 假设选择中首先需要考虑的问题是假设生成。构建翻译假设集合是假设选择的第一步，也是最重要的一步。理想的情况下，这个集合应该尽可能包含更多高质量的翻译假设，这样后面有更大的几率选出更好的结果。不过，由于单个模型的性能是有上限的，因此无法期望这些翻译假设的品质超越单个模型的上限。研究人员更加关心的是翻译假设的多样性，因为已经证明多样的翻译假设非常有助于提升系统融合的性能\upcite{DBLP:journals/corr/LiMJ16,xiao2013bagging}。为了生成多样的翻译假设，通常有两种思路：1）使用不同的模型生成翻译假设；2）使用同一个模型的不同参数和设置生成翻译假设。图\ref{fig:14-8} 展示了二者的区别。比如，可以使用基于循环神经网络的模型和Transformer模型生成不同的翻译假设，之后都放入集合中；也可以只用Transformer 模型，但是用不同的模型参数构建多个系统，之后分别生成翻译假设。在神经机器翻译中，经常采用的是第二种方式，因为系统开发的成本更低。

%----------------------------------------------
\begin{figure}[htp]
\centering
\input{./Chapter14/Figures/figure-hypothesis-generation}
\caption{多模型翻译假设生成 vs 单模型翻译假设生成}
\label{fig:14-8}
\end{figure}
%----------------------------------------------

\parinterval 此外，模型的选择也十分重要。所谓假设选择实际上就是要用一个更强的模型在候选中进行选择。这个“强” 模型一般是由更多、更复杂的子模型组合而成。常用的方法是直接使用翻译假设生成时的模型构建“强” 模型。比如，使用两个模型生成了翻译假设集合，之后对所有翻译假设都分别用这两个模型进行打分。最后，综合两个模型的打分（如线性插值）得到翻译假设的最终得分，并进行选择。当然，也可以使用更强大的统计模型对多个子模型进行组合，如使用更深、更宽的神经网络。

\parinterval 假设选择也可以被看作是一种简单的投票模型，对所有的候选用多个模型投票，选出最好的结果输出。包括重排序在内的很多方法也是假设选择的一种特例。比如，在重排序中，可以把生成$n$-best列表的过程看作是翻译假设生成过程，而重排序的过程可以被看作是融合多个子模型进行最终结果选择的过程。

%----------------------------------------------------------------------------------------
%    NEW SUBSUB-SECTION
%----------------------------------------------------------------------------------------

\subsection{局部预测融合}

\parinterval 神经机器翻译模型对每个目标语言位置$j$的单词的概率分布进行预测\footnote{即对于目标语言词汇表中的每个单词$w_r$，计算$\funp{P}(y_j=w_r | \seq{y}_{<j},\seq{x})$。}，假设有$K$个神经机器翻译系统，那么每个系统$k$都可以独立计算这个概率分布，记为$\funp{P}_{k} (y_j | \seq{y}_{<j},\seq{x})$。于是，可以用如下方式融合这$K$个系统的预测：
\begin{eqnarray}
\funp{P}(y_{j} | \seq{y}_{<j},\seq{x}) &=& \sum_{k=1}^K \gamma_{k} \cdot \funp{P}_{k} (y_j | \seq{y}_{<j},\seq{x})
\label{eq:14-11}
\end{eqnarray}

\noindent 其中，$\gamma_{k}$表示第$k$个系统的权重，且满足$\sum_{k=1}^{K} \gamma_{k} = 1$。权重$\{ \gamma_{k}\}$可以在开发集上自动调整，比如，使用最小错误率训练得到最优的权重（见\chapterseven）。不过在实践中发现，如果这$K$个模型都是由一个基础模型衍生出来的，权重$\{ \gamma_{k}\}$对最终结果的影响并不大。因此，有时候也简单的将权重设置为$\gamma_{k} = \frac{1}{K}$。图\ref{fig:14-9}展示了对三个模型预测结果的集成。

%----------------------------------------------
\begin{figure}[htp]
\centering
\input{./Chapter14/Figures/figure-forecast-result}
\caption{基于三个模型预测结果的集成}
\label{fig:14-9}
\end{figure}
%----------------------------------------------

\parinterval 公式\eqref{eq:14-11}是一种典型的线性插值模型，这类模型在语言建模等任务中已经得到成功应用。从统计学习的角度，多个模型的插值可以有效地降低经验错误率。不过，多模型集成依赖一个假设：这些模型之间需要有一定的互补性。这种互补性有时也体现在多个模型预测的上限上，称为Oracle。比如，可以把这$K$个模型输出中BLEU最高的结果作为Oracle，也可以选择每个预测结果中使BLEU 达到最高的译文单词，这样构成的句子作为Oracle。当然，并不是说Oracle 提高，模型集成的结果一定会变好。因为Oracle 是最理想情况下的结果，而实际预测的结果与Oracle 往往有很大差异。如何使用Oracle 进行模型优化也是很多研究人员在探索的问题。

\parinterval 此外，如何构建集成用的模型也是非常重要的，甚至说这部分工作会成为模型集成方法中最困难的部分\upcite{DBLP:conf/wmt/LiLXLLLWZXWFCLL19,Wang2018TencentNM,DBLP:conf/wmt/SennrichHB16}。为了增加模型的多样性，常用的方法有：

\begin{itemize}
\vspace{0.5em}
\item 改变模型宽度和深度，即用不同层数或者不同隐藏层大小得到多个模型；
\vspace{0.5em}
\item 使用不同的参数进行初始化，即用不同的随机种子初始化参数训练多个模型；
\vspace{0.5em}
\item 不同模型（局部）架构的调整，比如，使用不同的位置编码模型\upcite{Shaw2018SelfAttentionWR}、多层融合模型\upcite{WangLearning}等；
\vspace{0.5em}
\item 利用不同数量以及不同数据增强方式产生的伪数据训练模型\upcite{zhang-EtAl:2020:WMT}；
\vspace{0.5em}
\item 利用多分支多通道的模型，不同分支可能有不同结构，使得模型能有更好的表示能力\upcite{zhang-EtAl:2020:WMT}；
\vspace{0.5em}
\item 利用预训练进行参数共享之后微调的模型。
\vspace{0.5em}
\end{itemize}

%----------------------------------------------------------------------------------------
%    NEW SUBSUB-SECTION
%----------------------------------------------------------------------------------------

\subsection{译文重组}

\parinterval 假设选择是直接从已经生成的译文中进行选择，因此无法产生“新” 的译文，也就是它的输出只能是某个单模型的输出。此外，预测融合需要同时使用多个模型进行推断，对计算和内存消耗较大。而且这两种方法有一个共性问题：搜索都是基于一个个字符串，相比指数级的译文空间，所看到的结果还是非常小的一部分。对于这个问题，一种方法是利用更加紧凑的数据结构对指数级的译文串进行表示。比如，可以使用{\small\sffamily\bfseries{词格}}\index{词格}（Word Lattice\index{Word Lattice}）对多个译文串进行表示\upcite{DBLP:conf/emnlp/TrombleKOM08}。图\ref{fig:14-10}展示了基于$n$-best词串和基于词格的表示方法的区别。可以看到，词格中从起始状态到结束状态的每一条路径都表示一个译文，不同译文的不同部分可以通过词格中的节点得到共享\footnote{本例中的词格也是一个{\footnotesize\sffamily\bfseries{混淆网络}}\index{混淆网络}（Confusion Network\index{Confusion Network}）。}。理论上，词格可以把指数级数量的词串用线性复杂度的结构表示出来。

%----------------------------------------------------------------------
\begin{figure}[htp]
\centering
 \input{./Chapter14/Figures/figure-word-string-representation}
\caption{$n$-best词串表示 vs 基于词格的词串表示}
\label{fig:14-10}
\end{figure}
%----------------------------------------------------------------------
\parinterval 有了词格这样的结构，多模型集成又有了新的思路。首先，可以将多个模型的译文融合为词格。注意，这个词格会包含这些模型无法生成的完整译文句子。之后，用一个更强的模型在词格上搜索最优的结果。这个过程有可能找到一些“新”的译文，即结果可能是从多个模型的结果中重组而来的。词格上的搜索模型可以基于多模型的融合，也可以使用一个简单的模型，这里需要考虑的是将神经机器翻译模型适应到词格上进行推断\upcite{DBLP:conf/aaai/SuTXJSL17}。其过程基本与原始的模型推断没有区别，只是需要把模型预测的结果附着到词格中的每条边上，再进行推断。

%----------------------------------------------------------------------
\begin{figure}[htp]
\centering
 \input{./Chapter14/Figures/figure-different-integration-model}
\caption{不同的模型集成方法对比}
\label{fig:14-11}
\end{figure}
%----------------------------------------------------------------------

\parinterval 图\ref{fig:14-11}对比了不同模型集成方法的区别。从系统开发的角度看，假设选择和模型预测融合的复杂度较低，适合快速开发原型系统，而且性能稳定。译文重组需要更多的模块，系统调试的复杂度较高，但是由于看到了更大的搜索空间，因此系统性能提升的潜力较大\footnote{一般来说词格上的Oracle 要比$n$-best译文上的Oracle 的质量高。}。

%----------------------------------------------------------------------------------------
%    NEW SECTION
%----------------------------------------------------------------------------------------
\sectionnewpage
\section{小结与拓展阅读}

\parinterval 推断系统（或解码系统）是神经机器翻译的重要组成部分。在神经机器翻译研究中，单独针对推断问题开展的讨论并不多见。更多的工作是将其与实践结合，常见于开源系统、评测比赛中。但是，从应用的角度看，研发高效的推断系统是机器翻译能够被大规模使用的前提。本章也从神经机器翻译推断的基本问题出发，重点探讨了推断系统的效率、非自回归翻译、多模型集成等问题。但是，由于推断问题涉及的问题十分广泛，因此本章也无法对其进行全面覆盖。关于神经机器翻译模型推断还有以下若干研究方向值得关注：

\begin{itemize}
\vspace{0.5em}
\item 机器翻译系统中的推断也借用了{\small\sffamily\bfseries{统计推断}}\index{统计推断}（Statistical Inference）\index{Statistical Inference}的概念。传统意义上讲，这类方法都是在利用样本数据去推测总体的趋势和特征。因此，从统计学的角度也有很多不同的思路。例如，贝叶斯学习等方法就在自然语言处理中得到广泛应用\upcite{Held2013AppliedSI,Silvey2018StatisticalI}。其中比较有代表性的是{\small\sffamily\bfseries{变分方法}}\index{变分方法}（Variational Methods）\index{Variational Methods}。这类方法通过引入新的隐含变量来对样本的分布进行建模，从某种意义上说它是在描述“分布的分布”，因此这种方法对事物的统计规律描述得更加细致\upcite{Beal2003VariationalAF}。这类方法也被成功地用于统计机器翻译\upcite{Li2009VariationalDF,xiao2011language,}和神经机器翻译\upcite{Bastings2019ModelingLS,Shah2018GenerativeNM,Su2018VariationalRN,Zhang2016VariationalNM}。
\vspace{0.5em}
\item 推断系统也可以受益于更加高效的神经网络结构。这方面工作集中在结构化剪枝、减少模型的冗余计算、低秩分解等方向。结构化剪枝中的代表性工作是LayerDrop\upcite{DBLP:conf/iclr/FanGJ20,DBLP:conf/emnlp/WangXZ20,DBLP:journals/corr/abs-2002-02925}，这类方法在训练时随机选择部分子结构，在推断时根据输入来选择模型中的部分层进行计算，而跳过其余层，达到加速的目的。有关减少冗余计算的研究主要集中在改进注意力机制上，本章已经有所介绍。低秩分解则针对词向量或者注意力的映射矩阵进行改进，例如词频自适应表示\upcite{DBLP:conf/iclr/BaevskiA19}，词频越高则对应的向量维度越大，反之则越小，或者层数越高注意力映射矩阵维度越小\upcite{DBLP:journals/corr/abs-2006-04768,DBLP:journals/corr/abs-1911-12385,DBLP:journals/corr/abs-1906-09777,DBLP:conf/nips/YangLSL19}。在实践中比较有效的是较深的编码器与较浅的解码器结合的方式，极端情况下解码器仅使用1层神经网络即可取得与多层神经网络相媲美的翻译品质，从而极大地提升翻译效率\upcite{DBLP:journals/corr/abs-2006-10369,DBLP:conf/aclnmt/HuLLLLWXZ20,DBLP:journals/corr/abs-2010-02416}。在{\chapterfifteen}还会进一步对高效神经机器翻译的模型结构进行讨论。
\vspace{0.5em}
\item 在对机器翻译推断系统进行实际部署时，对存储的消耗也是需要考虑的因素。因此如何让模型变得更小也是研发人员所关注的方向。当前的模型压缩方法主要可以分为几类：剪枝、量化、知识蒸馏和轻量方法，其中轻量方法主要是基于更轻量模型结构的设计，这类方法已经在本章进行了介绍。剪枝主要包括权重大小剪枝\upcite{Han2015LearningBW,Lee2019SNIPSN,Frankle2019TheLT,Brix2020SuccessfullyAT}、 面向多头注意力的剪枝\upcite{Michel2019AreSH,DBLP:journals/corr/abs-1905-09418}、网络层以及其他结构剪枝等\upcite{Liu2017LearningEC,Liu2019RethinkingTV}，还有一些方法也通过在训练期间采用正则化的方式来提升剪枝能力\upcite{DBLP:conf/iclr/FanGJ20}。量化方法主要通过截断浮点数来减少模型的存储大小，使其仅使用几个比特位的数字表示方法便能存储整个模型，虽然会导致舍入误差，但压缩效果显著\upcite{DBLP:journals/corr/abs-1906-00532,Cheong2019transformersZ,Banner2018ScalableMF,Hubara2017QuantizedNN}。一些方法利用知识蒸馏手段还将Transformer模型蒸馏成如LSTMs 等其他各种推断速度更快的结构\upcite{Hinton2015Distilling,Munim2019SequencelevelKD,Tang2019DistillingTK}。
\vspace{0.5em}
\item 目前的翻译模型使用交叉熵损失作为优化函数，这在自回归模型上取得了非常优秀的性能。交叉熵是一个严格的损失函数，每个预测错误的单词所对应的位置都会受到惩罚，即使是编辑距离很小的输出序列\upcite{Ghazvininejad2020AlignedCE}。自回归模型会很大程度上避免这种惩罚，因为当前位置的单词是根据先前生成的词得到的，然而非自回归模型无法获得这种信息。如果在预测时漏掉一个单词，就可能会将正确的单词放在错误的位置上。为此，一些研究工作通过改进损失函数来提高非自回归模型的性能。一种做法使用一种新的交叉熵函数\upcite{Ghazvininejad2020AlignedCE}，它通过忽略绝对位置、关注相对顺序和词汇匹配来为非自回归模型提供更精确的训练信号。另外，也可以使用基于$n$-gram的训练目标\upcite{Shao2020MinimizingTB}来最小化模型与参考译文之间的$n$-gram差异。该训练目标在$n$-gram 的层面上评估预测结果，因此能够建模目标序列单词之间的依赖关系。
\vspace{0.5em}
\item 自回归模型解码时，当前位置单词的生成依赖于先前生成的单词，已生成的单词提供了较强的目标端上下文信息。与自回归模型相比，非自回归模型的解码器需要在信息更少的情况下执行翻译任务。一些研究工作通过将条件随机场引入非自回归模型中来对序列依赖进行建模\upcite{Ma2019FlowSeqNC}。也有工作引入了词嵌入转换矩阵来将源语言端的词嵌入转换为目标语言端的词嵌入来为解码器提供更好的输入\upcite{Guo2019NonAutoregressiveNM}。此外，研究人员也提出了轻量级的调序模块来显式地建模调序信息，以指导非自回归模型的推断\upcite{Ran2019GuidingNN}。
\vspace{0.5em}
\end{itemize}








